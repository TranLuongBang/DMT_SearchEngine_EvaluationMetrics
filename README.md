# DMT 

### Part 1.1 Search Engine Evaluation Metrics
Using Whoosh API to create 12 Search Engines by using different combinations of **Text-Analyzer** and **Scoring-Function**

The two different collections of documents are: Cranfield_DATASET and Time_DATASET. They consist of:
..) a set of html documents.
..) a set of queries.
..) a set of relevant documents identifiers only for SOME of the queries in the query set: the Ground-Truth.

**Evaluation Metrics**
- MRR
- P-Precision distribution table

For top-5 search engine, according to MRR evaluation metric, providing the following plots.
- P@k plot
- nDCG@k plot

According to nDCG@k plot, which is the best search engine configuration?

### Part 1.2
 Search Engine Selection Problem
 The Tech company need to select the best Search-Engine module, among 3 modules for the latest successful app. "AwsomeApp".
 To solve this problem, performing quantitative analysis to assess the quality of the diffirent Search-Engines, using only ground-truth ans their query-results to select the best one.
**Evaluation Metrics**
- Precision@k
- Recall@K
- nDCG@k

### Part 2.1 Near-Duplicate Detection
Dataset contains on **250K** songs.



 
 
 


